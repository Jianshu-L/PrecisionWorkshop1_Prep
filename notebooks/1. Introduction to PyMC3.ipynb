{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyMC3\n",
    "\n",
    "## Key objectives for PW#1 preparation\n",
    "- go through all probability distributions described as necessary\n",
    "- go through optional distributions if possible\n",
    "- get acquainted with distributions with coding exercises, change parameters\n",
    "- go through tutorial, examine models that were built in the tutorial\n",
    "\n",
    "### Reading group\n",
    "We will solve a very simple Bayesian inference problem analytically to demonstrate at the same time the speed of analytical solutions and the practical difficulty of finding and applying them. This problem will be connected to the Bayesian Clinical Trials paper we discussed a few weeks back in the Reading Group. This will motivate PyMC3 as a general \"inference machine\" which underlies the Probabilistic programming paradigm.\n",
    "\n",
    "### Instructions\n",
    "Do not worry if you don't understand everything (this is likely and expected) - the idea is to get acquainted with the workflow and understand the syntax at an introductory/high level.\n",
    "\n",
    "### Constructing a Bayesian model\n",
    "\n",
    "Let's briefly cover some ideas regarding Bayesian analysis using Markov chain Monte Carlo (MCMC) methods. You might wonder why a numerical simulation method like MCMC is the standard approach for fitting Bayesian models. \n",
    "\n",
    "Gelman et al. (2013) break down the business of Bayesian analysis into three primary steps:\n",
    "\n",
    "1. Specify a full probability model, including all parameters, data, transformations, missing values and predictions that are of interest.\n",
    "2. Calculate the posterior distribution of the unknown quantities in the model, conditional on the data.\n",
    "3. Perform model checking to evaluate the quality and suitablility of the model.\n",
    "\n",
    "While each of these steps is challenging, it is the second step that is the most difficult for non-trivial models, and was a bottleneck for the adoption of Bayesian methods for decades. \n",
    "\n",
    "### Bayesian Inference\n",
    "\n",
    "At this point, we should all be familiar with **Bayes Formula**:\n",
    "\n",
    "![bayes formula](images/bayes_formula.png)\n",
    "\n",
    "The equation expresses how our belief about the value of \\\\(\\theta\\\\), as expressed by the **prior distribution** \\\\(P(\\theta)\\\\) is reallocated following the observation of the data \\\\(y\\\\), as expressed by the posterior distribution the posterior distribution.\n",
    "\n",
    "Computing the posterior distribution is called the **inference problem**, and is usually the goal of Bayesian analysis.\n",
    "\n",
    "The innocuous denominator \\\\(Pr(y)\\\\) (the model **evidence**, or **marginal likelihood**) cannot be calculated directly, and is actually the expression in the numerator, integrated over all \\\\(\\theta\\\\):\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "\\\\[Pr(\\theta|y) = \\frac{Pr(y|\\theta)Pr(\\theta)}{\\int Pr(y|\\theta)Pr(\\theta) d\\theta}\\\\]\n",
    "</div>\n",
    "\n",
    "Computing this integral, which may involve many variables, is generally intractible with analytic methods. This is the major compuational hurdle for Bayesian analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning PyMC3\n",
    "In the following, we will use progressively more complicated examples as methods of exposing basic PyMC3 features.  **It is not the expectation that all parts of the modelling process and/or PyMC3 library will be fully understood.**  Rather a high-level feel should be built for how the modelling process works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Motivating Example: Linear Regression\n",
    "\n",
    "To introduce model definition, fitting and posterior analysis, we first consider a simple Bayesian linear regression model with normal priors for the parameters. We are interested in predicting outcomes $Y$ as normally-distributed observations with an expected value $\\mu$ that is a linear function of two predictor variables, $X_1$ and $X_2$.\n",
    "\n",
    "$$\\begin{aligned} \n",
    "Y  &\\sim \\mathcal{N}(\\mu, \\sigma^2) \\\\\n",
    "\\mu &= \\alpha + \\beta_1 X_1 + \\beta_2 X_2\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $\\alpha$ is the intercept, and $\\beta_i$ is the coefficient for covariate $X_i$, while $\\sigma$ represents the observation error. Since we are constructing a Bayesian model, the unknown variables in the model must be assigned a prior distribution. We choose zero-mean normal priors with variance of 100 for both regression coefficients, which corresponds to *weak* information regarding the true parameter values. We choose a half-normal distribution (normal distribution bounded at zero) as the prior for $\\sigma$.\n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\alpha &\\sim \\mathcal{N}(0, 100) \\\\\n",
    "\\beta_i &\\sim \\mathcal{N}(0, 100) \\\\\n",
    "\\sigma &\\sim \\lvert\\mathcal{N}(0, 1){\\rvert}\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Generating data\n",
    "\n",
    "We can simulate some artificial data from this model using only NumPy's `random` module, and then use PyMC3 to try to recover the corresponding parameters. We are intentionally generating the data to closely correspond the PyMC3 model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize random number generator\n",
    "np.random.seed(123)\n",
    "\n",
    "# True parameter values\n",
    "alpha, sigma = 1, 1\n",
    "beta = [1, 2.5]\n",
    "\n",
    "# Size of dataset\n",
    "size = 100\n",
    "\n",
    "# Predictor variable\n",
    "X1 = np.random.randn(size)\n",
    "X2 = np.random.randn(size) * 0.2\n",
    "\n",
    "# Simulate outcome variable\n",
    "Y = alpha + beta[0]*X1 + beta[1]*X2 + np.random.randn(size)*sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the simulated data look like. We use the `pylab` module from the plotting library matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,4))\n",
    "axes[0].scatter(X1, Y)\n",
    "axes[1].scatter(X2, Y)\n",
    "axes[0].set_ylabel('Y'); axes[0].set_xlabel('X1'); axes[1].set_xlabel('X2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "\n",
    "Specifying this model in PyMC3 is straightforward because the syntax is as close to the statistical notation. For the most part, each line of Python code corresponds to a line in the model notation above. \n",
    "\n",
    "First, we import PyMC. We use the convention of importing it as `pm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build our model, which we will present in full first, then explain each part line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = pm.Model()\n",
    "\n",
    "with basic_model:\n",
    "    \n",
    "    # Priors for unknown model parameters\n",
    "    alpha = pm.Normal('alpha', mu=0, sd=10)\n",
    "    beta = pm.Normal('beta', mu=0, sd=10, shape=2)\n",
    "    sigma = pm.HalfNormal('sigma', sd=1)\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = alpha + beta[0]*X1 + beta[1]*X2\n",
    "    \n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    Y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line,\n",
    "\n",
    "```python\n",
    "basic_model = Model()\n",
    "```\n",
    "\n",
    "creates a new `Model` object which is a container for the model random variables.\n",
    "\n",
    "Following instantiation of the model, the subsequent specification of the model components is performed inside a  `with` statement:\n",
    "\n",
    "```python\n",
    "with basic_model:\n",
    "```\n",
    "This creates a *context manager*, with our `basic_model` as the context, that includes all statements until the indented block ends. This means all PyMC3 objects introduced in the indented code block below the `with` statement are added to the model behind the scenes. Absent this context manager idiom, we would be forced to manually associate each of the variables with `basic_model` right after we create them. If you try to create a new random variable without a `with model:` statement, it will raise an error since there is no obvious model for the variable to be added to.\n",
    "\n",
    "The first three statements in the context manager:\n",
    "\n",
    "```python\n",
    "alpha = Normal('alpha', mu=0, sd=10)\n",
    "beta = Normal('beta', mu=0, sd=10, shape=2)\n",
    "sigma = HalfNormal('sigma', sd=1)\n",
    "```\n",
    "create a **stochastic** random variables with a Normal prior distributions for the regression coefficients with a mean of 0 and standard deviation of 10 for the regression coefficients, and a half-normal distribution for the standard deviation of the observations, $\\sigma$. These are stochastic because their values are partly determined by its parents in the dependency graph of random variables, which for priors are simple constants, and partly random (or stochastic). \n",
    "\n",
    "We call the `Normal` constructor to create a random variable to use as a normal prior. The first argument is always the *name* of the random variable, which should almost always match the name of the Python variable being assigned to, since it sometimes used to retrieve the variable from the model for summarizing output. The remaining required arguments for a stochastic object are the parameters, in this case `mu`, the mean, and `sd`, the standard deviation, which we assign hyperparameter values for the model. In general, a distribution's parameters are values that determine the location, shape or scale of the random variable, depending on the parameterization of the distribution. Most commonly used distributions, such as `Beta`, `Exponential`, `Categorical`, `Gamma`, `Binomial` and many others, are available in PyMC3.\n",
    "\n",
    "The `beta` variable has an additional `shape` argument to denote it as a vector-valued parameter of size 2. The `shape` argument is available for all distributions and specifies the length or shape of the random variable, but is optional for scalar variables, since it defaults to a value of one. It can be an integer, to specify an array, or a tuple, to specify a multidimensional array (*e.g.* `shape=(5,7)` makes random variable that takes on 5 by 7 matrix values). \n",
    "\n",
    "Detailed notes about distributions, sampling methods and other PyMC3 functions are available via the `help` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pm.Normal) #try help(Model), help(Uniform) or help(basic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the priors, the next statement creates the expected value `mu` of the outcomes, specifying the linear relationship:\n",
    "\n",
    "```python\n",
    "mu = alpha + beta[0]*X1 + beta[1]*X2\n",
    "```\n",
    "This creates a **deterministic** random variable, which implies that its value is *completely* determined by its parents' values. That is, there is no uncertainty beyond that which is inherent in the parents' values. Here, `mu` is just the sum of the intercept `alpha` and the two products of the coefficients in `beta` and the predictor variables, whatever their values may be. \n",
    "\n",
    "PyMC3 random variables and data can be arbitrarily added, subtracted, divided, multiplied together and indexed-into to create new random variables. This allows for great model expressivity. Many common mathematical functions like `sum`, `sin`, `exp` and linear algebra functions like `dot` (for inner product) and `inv` (for inverse) are also provided. \n",
    "\n",
    "The final line of the model, defines `Y_obs`, the sampling distribution of the outcomes in the dataset.\n",
    "\n",
    "```python\n",
    "Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)\n",
    "```\n",
    "\n",
    "This is a special case of a stochastic variable that we call an **observed stochastic**, and represents the data likelihood of the model. It is identical to a standard stochastic, except that its `observed` argument, which passes the data to the variable, indicates that the values for this variable were observed, and should not be changed by any fitting algorithm applied to the model. The data can be passed in the form of either a `numpy.ndarray` or `pandas.DataFrame` object.\n",
    "\n",
    "Notice that, unlike for the priors of the model, the parameters for the normal distribution of `Y_obs` are not fixed values, but rather are the deterministic object `mu` and the stochastic `sigma`. This creates parent-child relationships between the likelihood and these two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "\n",
    "Having completely specified our model, the next step is to obtain posterior estimates for the unknown variables in the model. Ideally, we could calculate the posterior estimates analytically, but for most non-trivial models, this is not feasible. We will consider two approaches, whose appropriateness depends on the structure of the model and the goals of the analysis: finding the *maximum a posteriori* (MAP) point using optimization methods, and computing summaries based on samples drawn from the posterior distribution using Markov Chain Monte Carlo (MCMC) sampling methods.\n",
    "\n",
    "#### Maximum a posteriori methods\n",
    "\n",
    "The **maximum a posteriori (MAP)** estimate for a model, is the mode of the posterior distribution and is generally found using numerical optimization methods. This is often fast and easy to do, but only gives a point estimate for the parameters and can be biased if the mode isn't representative of the distribution. PyMC3 provides this functionality with the `find_MAP` function.\n",
    "\n",
    "Below we find the MAP for our original model. The MAP is returned as a parameter **point**, which is always represented by a Python dictionary of variable names to NumPy arrays of parameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimate = pm.find_MAP(model=basic_model)\n",
    "    \n",
    "map_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `find_MAP` uses the Broyden–Fletcher–Goldfarb–Shanno (BFGS) optimization algorithm to find the maximum of the log-posterior but also allows selection of other optimization algorithms from the `scipy.optimize` module. For example, below we use Powell's method to find the MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "map_estimate = pm.find_MAP(model=basic_model, fmin=optimize.fmin_powell)\n",
    "    \n",
    "map_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the MAP estimate is not always reasonable, especially if the mode is at an extreme. This can be a subtle issue; with high dimensional posteriors, one can have areas of extremely high density but low total probability because the volume is very small. This will often occur in hierarchical models with the variance parameter for the random effect. If the individual group means are all the same, the posterior will have near infinite density if the scale parameter for the group means is almost zero, even though the probability of such a small scale parameter will be small since the group means must be extremely close together. \n",
    "\n",
    "Most techniques for finding the MAP estimate also only find a *local* optimum (which is often good enough), but can fail badly for multimodal posteriors if the different modes are meaningfully different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling methods\n",
    "\n",
    "Though finding the MAP is a fast and easy way of obtaining estimates of the unknown model parameters, it is limited because there is no associated estimate of uncertainty produced with the MAP estimates. Instead, a simulation-based approach such as Markov chain Monte Carlo (MCMC) can be used to obtain a Markov chain of values that, given the satisfaction of certain conditions, are indistinguishable from samples from the posterior distribution.\n",
    "\n",
    "**Note** that the sampler will by default sample 1500 samples and **throw away the first 500 samples**, as they are considered 'burn-in' samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with basic_model:\n",
    "    # draw posterior samples\n",
    "    trace = pm.sample(draws=2000, tune=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample` function runs the samping methods for the given number of iterations and returns a `Trace` object containing the samples collected, in the order they were collected. The `trace` object can be queried in a similar way to a `dict` containing a map from variable names to `numpy.array`s. The first dimension of the array is the sampling index and the later dimensions match the shape of the variable. We can see the last 5 values for the `alpha` variable as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace['alpha'][-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior analysis\n",
    "`PyMC3` provides plotting and summarization functions for inspecting the sampling output. A simple posterior plot can be created using `traceplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left column consists of a smoothed histogram (using kernel density estimation) of the marginal posteriors of each stochastic random variable while the right column contains the samples of the Markov chain plotted in sequential order. The `beta` variable, being vector-valued, produces two histograms and two sample traces, corresponding to both predictor coefficients.\n",
    "\n",
    "In addition, the `summary` function provides a text-based output of common posterior statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PW Exercise\n",
    "Can you reduce the sampling so the results are totally different? Note, the trace contains all the samples, so just go ahead and use indexing to reduce the number of samples used for posterior plot (and use samples from start) instead of going back and changing the sampling initially.\n",
    "\n",
    "- Can you make the graph on the right clearly irregular?\n",
    "- When you compare posterior estimates for the betas in your traceplot below and in the (stabilized) traceplot above, can you produce a large difference?\n",
    "- Where are the differences you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example: Coal mining disasters\n",
    "\n",
    "Consider the following time series of recorded coal mining disasters in the UK from 1851 to 1962 (Jarrett, 1979). The number of disasters is thought to have been affected by changes in safety regulations during this period.\n",
    "\n",
    "Let's build a model for this series and attempt to estimate when the change occurred. First we plot the data and specify the form of the model mathematically and proceed to encoded it in PyMC3 and run inference.\n",
    "\n",
    "This model is more complicated than the previous GLM model and shows more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disasters_data = np.array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n",
    "                         3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n",
    "                         2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3, 0, 0,\n",
    "                         1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n",
    "                         0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n",
    "                         3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n",
    "                         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n",
    "\n",
    "n_years = len(disasters_data)\n",
    "\n",
    "plt.figure(figsize=(12.5, 3.5))\n",
    "plt.bar(np.arange(1851, 1962), disasters_data, color=\"#348ABD\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Disasters\")\n",
    "plt.title(\"UK coal mining disasters, 1851-1962\")\n",
    "plt.xlim(1851, 1962);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent our conceptual model formally as a statistical model:\n",
    "\n",
    "$$\\begin{array}{ccc}  \n",
    "(y_t | \\tau, \\lambda_1, \\lambda_2) \\sim\\text{Poisson}\\left(r_t\\right), & r_t=\\left\\{\n",
    "\\begin{array}{lll}             \n",
    "\\lambda_1 &\\text{if}& t< \\tau\\\\ \n",
    "\\lambda_2 &\\text{if}& t\\ge \\tau             \n",
    "\\end{array}\\right.,&t\\in[t_l,t_h]\\\\         \n",
    "\\tau \\sim \\text{DiscreteUniform}(t_l, t_h)\\\\         \n",
    "\\lambda_1\\sim \\text{Exponential}(a)\\\\         \n",
    "\\lambda_2\\sim \\text{Exponential}(b)     \n",
    "\\end{array}$$\n",
    "\n",
    "Because we have defined $y$ by its dependence on $\\tau$, $\\lambda_1$ and $\\lambda_2$, the latter three are known as the *parents* of $y$, and $y$ is called their *child*. Similarly, the parents of $\\tau$ are $t_l$ and $t_h$, and $\\tau$ is the child of $t_l$ and $t_h$.\n",
    "\n",
    "## Implementing a PyMC Model\n",
    "\n",
    "At the model-specification stage (before the data are observed), $y$, $\\tau$, $\\lambda_1$, and $\\lambda_2$ are all random variables. Bayesian \"random\" variables have not necessarily arisen from a physical random process. The Bayesian interpretation of probability is **epistemic**, meaning random variable $x$'s probability distribution $p(x)$ represents our knowledge and uncertainty about $x$'s value. Candidate values of $x$ for which $p(x)$ is high are relatively more probable, given what we know. \n",
    "\n",
    "We can generally divide the variables in a Bayesian model into two types: **stochastic** and **deterministic**. The only deterministic variable in this model is $r$. If we knew the values of $r$'s parents, we could compute the value of $r$ exactly. A deterministic like $r$ is defined by a mathematical function that returns its value given values for its parents. Deterministic variables are sometimes called the *systemic* part of the model. The nomenclature is a bit confusing, because these objects usually represent random variables; since the parents of $r$ are random, $r$ is random also.\n",
    "\n",
    "On the other hand, even if the values of the parents of variables `switchpoint`, `disasters` (before observing the data), `early_mean` or `late_mean` were known, we would still be uncertain of their values. These variables are stochastic, characterized by probability distributions that express how plausible their candidate values are, given values for their parents.\n",
    "\n",
    "Let's begin by defining the unknown switchpoint as a discrete uniform random variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import DiscreteUniform, Model\n",
    "\n",
    "with Model() as disaster_model:\n",
    "\n",
    "    switchpoint = DiscreteUniform('switchpoint', lower=0, upper=n_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done two things here. First, we have created a `Model` object; a `Model` is a Python object that encapsulates all of the variables that comprise our theoretical model, keeping them in a single container so that they may be used as a unit. After a `Model` is created, we can populate it with all of the model components that we specified when we wrote the model down. \n",
    "\n",
    "Notice that the `Model` above was declared using a `with` statement. This expression is used to define a Python idiom known as a **context manager**. Context managers, in general, are used to manage resources of some kind within a program. In this case, our resource is a `Model`, and we would like to add variables to it so that we can fit our statistical model. The key characteristic of the context manager is that the resources it manages are only defined within the indented block corresponding to the `with` statement. PyMC uses this idiom to automatically add defined variables to a model. Thus, any variable we define is automatically added to the `Model`, without having to explicitly add it. This avoids the repetitive syntax of `add` methods/functions that you see in some machine learning packages:\n",
    "\n",
    "```python\n",
    "model.add(a_variable)\n",
    "model.add(another_variable)\n",
    "model.add(yet_another_variable)\n",
    "model.add(and_again)\n",
    "model.add(please_kill_me_now)\n",
    "...\n",
    "```\n",
    "\n",
    "In fact, PyMC variables cannot be defined without a corresponding `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove comment to show error\n",
    "#oops = DiscreteUniform('oops', lower=0, upper=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, variables can be explicitly added to models without the use of a context manager, via the variable's optional `model` argument.\n",
    "\n",
    "```python\n",
    "disaster_model = Model()\n",
    "switchpoint = DiscreteUniform('switchpoint', lower=0, upper=110, model=disaster_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability mass/density functions implemented in PyMC3\n",
    "\n",
    "PyMC3 includes most of the common random variable **distributions** used for statistical modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import distributions\n",
    "distributions.__all__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By having a library of variables that represent statistical distributions, users are relieved of having to code distrbutions themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can create the exponentially-distributed variables `early_mean` and `late_mean` for the early and late Poisson rates, respectively (also in the context of the model `distater_model`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Exponential\n",
    "\n",
    "with disaster_model:\n",
    "    \n",
    "    early_mean = Exponential('early_mean', 1)\n",
    "    late_mean = Exponential('late_mean', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the variable `rate`, which selects the early rate `early_mean` for times before `switchpoint` and the late rate `late_mean` for times after `switchpoint`. We create `rate` using the `switch` function, which returns `early_mean` when the switchpoint is larger than (or equal to) a particular year, and `late_mean` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3.math import switch\n",
    "\n",
    "with disaster_model:\n",
    "    \n",
    "    rate = switch(switchpoint >= np.arange(n_years), early_mean, late_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define the **data likelihood**, or sampling distribution. In this case, our measured outcome is the number of disasters in each year, `disasters`. This is a stochastic variable but unlike `early_mean` and `late_mean` we have *observed* its value. To express this, we set the argument `observed` to the observed sequence of disasters. This tells PyMC that this distribution's value is fixed, and should not be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Poisson\n",
    "\n",
    "with disaster_model:\n",
    "    \n",
    "    disasters = Poisson('disasters', mu=rate, observed=disasters_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Why are data and unknown variables represented by the same object?\n",
    "\n",
    ">Since its represented by PyMC random variable object, `disasters` is defined by its dependence on its parent `rate` even though its value is **fixed**. This isn't just a quirk of PyMC's syntax; Bayesian hierarchical notation itself makes no distinction between random variables and data. The reason is simple: to use Bayes' theorem to compute the posterior, we require the likelihood. Even though `disasters`'s value is known and fixed, we need to formally assign it a *probability distribution* as if it were a random variable. Remember, the likelihood and the probability function are essentially the same, except that the former is regarded as a function of the parameters and the latter as a function of the data. This point can be counterintuitive at first, as many peoples' instinct is to regard data as fixed a priori and unknown variables as dependent on the data. \n",
    "\n",
    "> One way to understand this is to think of statistical models as predictive models for data, or as models of the processes that gave rise to data. Before observing the value of `disasters`, we could have sampled from its prior predictive distribution $p(y)$ (*i.e.* the marginal distribution of the data) as follows:\n",
    "\n",
    "> -   Sample `early_mean`, `switchpoint` and `late_mean` from their\n",
    ">     priors.\n",
    "> -   Sample `disasters` conditional on these values.\n",
    "\n",
    "> Even after we observe the value of `disasters`, we need to use this process model to make inferences about `early_mean` , `switchpoint` and `late_mean` because its the only information we have about how the variables are related.\n",
    "\n",
    "> We will see later that we can sample from this fixed stochastic random variable, to obtain predictions after having observed our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we specified at the top of the page has now been fully implemented in PyMC3. Let's have a look at the model's attributes to see what we have.\n",
    "\n",
    "The stochastic nodes in the model are identified in the `vars` (*i.e.* variables) attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disaster_model.vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two variables are the log-transformed versions of the early and late rate parameters. In PyMC3, variables with purely positive priors like `Exponential` are **transformed** with a log function. This makes sampling more robust. Behind the scenes, a variable in the unconstrained space (named `<variable name>_log_`) is added to the model for sampling. In this model this happens behind the scenes. Variables with priors that constrain them on two sides, like `Beta` or `Uniform` (continuous), are also transformed to be unconstrained but with a log odds transform.\n",
    "\n",
    "The original variables have become deterministic nodes in the model, since they only represent values that have been back-transformed from the transformed variable, which has been subject to fitting or sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disaster_model.deterministics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder why `rate`, which is a deterministic component of the model, is not in this list. This is because, unlike the other components of the model, `rate` has not been given a name and given a formal PyMC data structure. It is essentially an **intermediate calculation** in the model, implying that we are not interested in its value when it comes to summarizing the output from the model. Most PyMC objects have a name assigned; these names are used for storage and post-processing:\n",
    "\n",
    "-   as keys in on-disk databases,\n",
    "-   as axis labels in plots of traces,\n",
    "-   as table labels in summary statistics.\n",
    "\n",
    "If we wish to include `rate` in our output, we need to make it a `Deterministic` object, and give it a name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Deterministic\n",
    "\n",
    "with disaster_model:\n",
    "    \n",
    "    rate = Deterministic('rate', switch(switchpoint >= np.arange(n_years), early_mean, late_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `rate` is included in the `Model`'s deterministics list, and the model will retain its samples during MCMC sampling, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disaster_model.deterministics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMC3 Variables\n",
    "\n",
    "Each of the built-in statistical variables are subclasses of the generic `Distribution` class in PyMC3. The `Distribution` carries relevant **attributes** about the probability distribution, such as the data type (called `dtype`), any relevant transformations (`transform`, see below), and initial values (`init_value`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disasters.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_mean.init_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC's built-in distribution variables can also be used to generate **random values** from that variable. For example, the `switchpoint`, which is a discrete uniform random variable, can generate random draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(switchpoint.random(size=100000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noted earlier, some variables have undergone **transformations** prior to sampling. Such variables will have `transformed` attributes that points to the variable that it has been transformed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_mean.transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables will usually have an associated distribution, as determined by the constructor used to create it. For example, the `switchpoint` variable was created by calling `DiscreteUniform()`. Hence, its distribution is `DiscreteUniform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "switchpoint.distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all Python objects, the underlying type of a variable can be exposed with the `type()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(switchpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(disasters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn more about these types in an upcoming section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable log-probabilities\n",
    "\n",
    "All PyMC3 stochastic variables can evaluate their probability mass or density functions at a particular value, given the values of their parents. The **logarithm** of a stochastic object's probability mass or density can be\n",
    "accessed via the `logp` method.\n",
    "\n",
    "Note: the PyMC3 stochastic variables don't implement mass/density at all, only the logarithm.  An exponent must be applied to get the original density/mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "switchpoint.logp({'switchpoint':15, 'early_mean_log__':1, 'late_mean_log__':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **vector-valued** variables like `disasters`, the `logp` attribute returns the **sum** of the logarithms of\n",
    "the joint probability or density of all elements of the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disasters.logp({'switchpoint':55, 'early_mean_log__':1, 'late_mean_log__':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model with MCMC\n",
    "\n",
    "PyMC3's `sample` function will fit probability models (linked collections of variables) like ours using Markov chain Monte Carlo (MCMC) sampling. Unless we manually assign particular algorithms to variables in our model, PyMC will assign algorithms that it deems appropriate (it usually does a decent job of this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    trace = sample(2000, init=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the Markov chain of draws from the model in a data structure called a **trace**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample()` function always takes at least one argument, `draws`, which specifies how many samples to draw. However, there are a number of additional optional arguments that are worth knowing about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `step` argument is what allows users to manually override the sampling algorithms used to fit the model. For example, if we wanted to use a **slice sampler** to sample the `early_mean` and `late_mean` variables, we could specify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Slice\n",
    "\n",
    "with disaster_model:\n",
    "    step_trace = sample(draws=1000, tune=500, step=Slice(vars=[early_mean, late_mean]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the samples\n",
    "\n",
    "The output of the `sample` function is a `MultiTrace` object, which stores the sequence of samples for each variable in the model. These traces can be accessed using dict-style indexing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trace['late_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace can also be sliced using the NumPy array slice `[start:stop:step]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace['late_mean', -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace['late_mean', ::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling output\n",
    "\n",
    "You can examine the marginal posterior of any variable by plotting a\n",
    "histogram of its trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(trace['late_mean']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC has its own plotting functionality dedicated to plotting MCMC output. For example, we can obtain a time series plot of the trace and a histogram using `traceplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import traceplot\n",
    "\n",
    "traceplot(trace[500:], varnames=['early_mean', 'late_mean', 'switchpoint']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper left-hand pane of each figure shows the temporal series of the\n",
    "samples from each parameter, while below is an autocorrelation plot of\n",
    "the samples. The right-hand pane shows a histogram of the trace. The\n",
    "trace is useful for evaluating and diagnosing the algorithm's\n",
    "performance, while the histogram is useful for\n",
    "visualizing the posterior.\n",
    "\n",
    "For a non-graphical summary of the posterior, simply call the `stats` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import summary\n",
    "\n",
    "summary(trace[500:], varnames=['early_mean', 'late_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PW Exercise\n",
    "Here we have more detail about the sample function and we have observed its output in detail. Important parameters for us now are **draws** and **tune**, which affect how many samples are drawn and how long the chain runs to stabilize before beginning sampling.\n",
    "\n",
    "- Note that the model contains a switchpoint variable and is not end-to-end differntiable.\n",
    "- Play around with the paraemters (hint: reduce both draws, tune) and re-run the cells above.\n",
    "- Observe warnings emitted by the internal diagnostics of PyMC3.\n",
    "- Observe how the output changes when you sample more/less.\n",
    "- Get a feel for the variability in this model.\n",
    "- No need to dig deeper into this or try to study details - we will do that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PW Revisiting probability distributions\n",
    "Let us revisit the list of distributions above and examine them in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distributions.__all__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important distributions\n",
    "Please make sure you're familiar with the form and parameters of the following distributions:\n",
    "- Uniform\n",
    "- Normal\n",
    "- Exponential\n",
    "- Poisson\n",
    "- Beta\n",
    "- Binomial\n",
    "- Bernoulli\n",
    "- NegativeBinomial\n",
    "- Multinomial\n",
    "- Gamma\n",
    "- StudentT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate log-probability from a given distribution (without using a model)\n",
    "# http://docs.pymc.io/prob_dists.html#using-pymc-distributions-without-a-model\n",
    "from pymc3 import plots, Normal\n",
    "y = Normal.dist(mu=0, sd=1)\n",
    "plots.kdeplot(y.random(size=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scipy.stats to plot distributions and histograms\n",
    "\n",
    "PyMC3 offers support for these distributions but you will find it's heavily geared toward inference.  For example there is no density function (continuous) and no probability mass function (discrete), just the log-probability.  Below is an example for the normal distribution - observe that the KDE (=kernel density estimate) plot is not especially accurate.\n",
    "\n",
    "Therefore to investige the distributions, you may find it easier to use [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html) as shown below the examples using pymc3.\n",
    "\n",
    "However, if you are running short on time, it's fine to look at wikipedia to get acquainted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from here: https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.stats.norm.html\n",
    "# NOTE: scipy stats uses weird names for parameters in the distributions, for example below for the normal distribution,\n",
    "# we have that 'loc' is mean value and 'scale' is standard deviation (sigma).  One needs to look into the documentation\n",
    "# in scipy.stats\n",
    "# \n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.gca()\n",
    "#fit, ax = plt.subplots(1, 1)\n",
    "legend = []\n",
    "for sigma in [1, 3, 5]:\n",
    "    x = np.linspace(norm.ppf(0.01,scale=sigma), norm.ppf(0.99,scale=sigma), 100)\n",
    "    ax.plot(x, norm.pdf(x,scale=sigma), lw=3, alpha=0.6, label='norm pdf')\n",
    "    legend.append('$\\sigma = %g$' % sigma)\n",
    "plt.legend(legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete example: binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10 # our choice of number of tries for the binomial distribution\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.gca()\n",
    "x = range(11) # we know the universe of possible values is 0..10\n",
    "\n",
    "legend = []\n",
    "for p in [0.1, 0.3, 0.6, 0.8]:\n",
    "    ax.plot(x, binom.pmf(x, n=n, p=p), lw=3, alpha=0.3, label='norm pdf')\n",
    "    r = binom.rvs(n=n, p=p, size=1000)\n",
    "    legend.append('$p=%g$' % p)\n",
    "plt.legend(legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PW exercise\n",
    "Edit the above plots with more distributions from the selected set above to get a feel for them.  As stated above, Wikipedia is a fine plan B if you find it difficult to use the (non-intuitive) scipy package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Salvatier, J., Wiecki, T. V., & Fonnesbeck, C. (2016). Probabilistic programming in Python using PyMC3. PeerJ Computer Science, 2(2), e55. http://doi.org/10.7717/peerj-cs.55"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
